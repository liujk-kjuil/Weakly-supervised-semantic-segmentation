2025-01-12 15:34:33,154 - Arguments:
2025-01-12 15:34:33,154 - batch_size: 64
2025-01-12 15:34:33,154 - max_epoches: 10
2025-01-12 15:34:33,154 - network: network.resnet38_cls
2025-01-12 15:34:33,154 - lr: 0.005
2025-01-12 15:34:33,154 - num_workers: 10
2025-01-12 15:34:33,154 - wt_dec: 0.0005
2025-01-12 15:34:33,154 - session_name: Stage 1
2025-01-12 15:34:33,154 - env_name: PDA
2025-01-12 15:34:33,154 - model_name: PDA
2025-01-12 15:34:33,154 - n_class: 4
2025-01-12 15:34:33,154 - weights: init_weights/ilsvrc-cls_rna-a1_cls1000_ep-0001.params
2025-01-12 15:34:33,154 - trainroot: datasets/LUAD-HistoSeg/train/
2025-01-12 15:34:33,154 - testroot: datasets/LUAD-HistoSeg/test/
2025-01-12 15:34:33,154 - save_folder: checkpoints/
2025-01-12 15:34:33,154 - init_gama: 1
2025-01-12 15:34:33,154 - dataset: luad
2025-01-12 15:34:33,154 - log_dir: logs/stage1/exp09
2025-01-12 15:34:33,154 - ----------
2025-01-12 15:34:38,981 - Session started:
2025-01-12 15:34:38,982 - Sun Jan 12 15:34:38 2025
2025-01-12 15:35:47,778 - Epoch: 0
2025-01-12 15:35:47,778 - Iter:  100/ 2600
2025-01-12 15:35:47,778 - Loss:0.3379
2025-01-12 15:35:47,778 - avg_ep_EM:0.5656
2025-01-12 15:35:47,778 - avg_ep_acc:0.7453
2025-01-12 15:35:47,778 - lr: 0.004828
2025-01-12 15:35:47,778 - Fin:Sun Jan 12 16:04:27 2025
2025-01-12 15:35:47,778 - ----------
2025-01-12 15:36:54,457 - Epoch: 0
2025-01-12 15:36:54,458 - Iter:  200/ 2600
2025-01-12 15:36:54,458 - Loss:0.2525
2025-01-12 15:36:54,458 - avg_ep_EM:0.6315
2025-01-12 15:36:54,458 - avg_ep_acc:0.8020
2025-01-12 15:36:54,458 - lr: 0.004654
2025-01-12 15:36:54,458 - Fin:Sun Jan 12 16:04:00 2025
2025-01-12 15:36:54,458 - ----------
2025-01-12 15:37:34,590 - Gama of progressive dropout attention is: 0.98000000
2025-01-12 15:38:03,659 - Epoch: 1
2025-01-12 15:38:03,659 - Iter:  300/ 2600
2025-01-12 15:38:03,659 - Loss:0.2314
2025-01-12 15:38:03,659 - avg_ep_EM:0.6660
2025-01-12 15:38:03,659 - avg_ep_acc:0.8281
2025-01-12 15:38:03,659 - lr: 0.004479
2025-01-12 15:38:03,659 - Fin:Sun Jan 12 16:04:12 2025
2025-01-12 15:38:03,660 - ----------
2025-01-12 15:39:10,379 - Epoch: 1
2025-01-12 15:39:10,379 - Iter:  400/ 2600
2025-01-12 15:39:10,379 - Loss:0.2077
2025-01-12 15:39:10,379 - avg_ep_EM:0.7088
2025-01-12 15:39:10,379 - avg_ep_acc:0.8565
2025-01-12 15:39:10,379 - lr: 0.004304
2025-01-12 15:39:10,379 - Fin:Sun Jan 12 16:04:03 2025
2025-01-12 15:39:10,379 - ----------
2025-01-12 15:40:17,183 - Epoch: 1
2025-01-12 15:40:17,183 - Iter:  500/ 2600
2025-01-12 15:40:17,183 - Loss:0.2044
2025-01-12 15:40:17,183 - avg_ep_EM:0.7169
2025-01-12 15:40:17,183 - avg_ep_acc:0.8605
2025-01-12 15:40:17,183 - lr: 0.004127
2025-01-12 15:40:17,183 - Fin:Sun Jan 12 16:03:57 2025
2025-01-12 15:40:17,183 - ----------
2025-01-12 15:40:30,731 - Gama of progressive dropout attention is: 0.96040000
2025-01-12 15:41:26,657 - Epoch: 2
2025-01-12 15:41:26,657 - Iter:  600/ 2600
2025-01-12 15:41:26,657 - Loss:0.1870
2025-01-12 15:41:26,657 - avg_ep_EM:0.7471
2025-01-12 15:41:26,657 - avg_ep_acc:0.8766
2025-01-12 15:41:26,657 - lr: 0.003950
2025-01-12 15:41:26,657 - Fin:Sun Jan 12 16:04:05 2025
2025-01-12 15:41:26,657 - ----------
2025-01-12 15:42:33,413 - Epoch: 2
2025-01-12 15:42:33,413 - Iter:  700/ 2600
2025-01-12 15:42:33,413 - Loss:0.1786
2025-01-12 15:42:33,413 - avg_ep_EM:0.7475
2025-01-12 15:42:33,413 - avg_ep_acc:0.8774
2025-01-12 15:42:33,413 - lr: 0.003772
2025-01-12 15:42:33,413 - Fin:Sun Jan 12 16:04:01 2025
2025-01-12 15:42:33,413 - ----------
2025-01-12 15:43:26,986 - Gama of progressive dropout attention is: 0.94119200
2025-01-12 15:43:43,291 - Epoch: 3
2025-01-12 15:43:43,291 - Iter:  800/ 2600
2025-01-12 15:43:43,291 - Loss:0.1834
2025-01-12 15:43:43,291 - avg_ep_EM:0.7521
2025-01-12 15:43:43,291 - avg_ep_acc:0.8809
2025-01-12 15:43:43,291 - lr: 0.003593
2025-01-12 15:43:43,291 - Fin:Sun Jan 12 16:04:07 2025
2025-01-12 15:43:43,291 - ----------
2025-01-12 15:44:53,339 - Epoch: 3
2025-01-12 15:44:53,339 - Iter:  900/ 2600
2025-01-12 15:44:53,339 - Loss:0.1858
2025-01-12 15:44:53,339 - avg_ep_EM:0.7515
2025-01-12 15:44:53,340 - avg_ep_acc:0.8803
2025-01-12 15:44:53,340 - lr: 0.003413
2025-01-12 15:44:53,340 - Fin:Sun Jan 12 16:04:13 2025
2025-01-12 15:44:53,340 - ----------
2025-01-12 15:46:03,429 - Epoch: 3
2025-01-12 15:46:03,430 - Iter: 1000/ 2600
2025-01-12 15:46:03,430 - Loss:0.1716
2025-01-12 15:46:03,430 - avg_ep_EM:0.7593
2025-01-12 15:46:03,430 - avg_ep_acc:0.8840
2025-01-12 15:46:03,430 - lr: 0.003232
2025-01-12 15:46:03,430 - Fin:Sun Jan 12 16:04:18 2025
2025-01-12 15:46:03,430 - ----------
2025-01-12 15:46:31,629 - Gama of progressive dropout attention is: 0.92236816
2025-01-12 15:47:15,991 - Epoch: 4
2025-01-12 15:47:15,992 - Iter: 1100/ 2600
2025-01-12 15:47:15,992 - Loss:0.1695
2025-01-12 15:47:15,992 - avg_ep_EM:0.7638
2025-01-12 15:47:15,992 - avg_ep_acc:0.8885
2025-01-12 15:47:15,992 - lr: 0.003050
2025-01-12 15:47:15,992 - Fin:Sun Jan 12 16:04:28 2025
2025-01-12 15:47:15,992 - ----------
2025-01-12 15:48:26,164 - Epoch: 4
2025-01-12 15:48:26,164 - Iter: 1200/ 2600
2025-01-12 15:48:26,164 - Loss:0.1629
2025-01-12 15:48:26,164 - avg_ep_EM:0.7767
2025-01-12 15:48:26,164 - avg_ep_acc:0.8940
2025-01-12 15:48:26,164 - lr: 0.002866
2025-01-12 15:48:26,164 - Fin:Sun Jan 12 16:04:31 2025
2025-01-12 15:48:26,165 - ----------
2025-01-12 15:49:36,097 - Epoch: 4
2025-01-12 15:49:36,097 - Iter: 1300/ 2600
2025-01-12 15:49:36,097 - Loss:0.1544
2025-01-12 15:49:36,097 - avg_ep_EM:0.7839
2025-01-12 15:49:36,097 - avg_ep_acc:0.8973
2025-01-12 15:49:36,097 - lr: 0.002681
2025-01-12 15:49:36,097 - Fin:Sun Jan 12 16:04:33 2025
2025-01-12 15:49:36,097 - ----------
2025-01-12 15:49:36,276 - Gama of progressive dropout attention is: 0.90392080
2025-01-12 15:50:48,536 - Epoch: 5
2025-01-12 15:50:48,536 - Iter: 1400/ 2600
2025-01-12 15:50:48,536 - Loss:0.1490
2025-01-12 15:50:48,536 - avg_ep_EM:0.8012
2025-01-12 15:50:48,536 - avg_ep_acc:0.9064
2025-01-12 15:50:48,536 - lr: 0.002495
2025-01-12 15:50:48,536 - Fin:Sun Jan 12 16:04:39 2025
2025-01-12 15:50:48,536 - ----------
2025-01-12 15:51:58,661 - Epoch: 5
2025-01-12 15:51:58,661 - Iter: 1500/ 2600
2025-01-12 15:51:58,661 - Loss:0.1480
2025-01-12 15:51:58,661 - avg_ep_EM:0.8006
2025-01-12 15:51:58,661 - avg_ep_acc:0.9064
2025-01-12 15:51:58,661 - lr: 0.002307
2025-01-12 15:51:58,661 - Fin:Sun Jan 12 16:04:41 2025
2025-01-12 15:51:58,661 - ----------
2025-01-12 15:52:40,643 - Gama of progressive dropout attention is: 0.88584238
2025-01-12 15:53:10,949 - Epoch: 6
2025-01-12 15:53:10,950 - Iter: 1600/ 2600
2025-01-12 15:53:10,950 - Loss:0.1506
2025-01-12 15:53:10,950 - avg_ep_EM:0.7974
2025-01-12 15:53:10,950 - avg_ep_acc:0.9047
2025-01-12 15:53:10,950 - lr: 0.002118
2025-01-12 15:53:10,950 - Fin:Sun Jan 12 16:04:45 2025
2025-01-12 15:53:10,950 - ----------
2025-01-12 15:54:21,186 - Epoch: 6
2025-01-12 15:54:21,186 - Iter: 1700/ 2600
2025-01-12 15:54:21,186 - Loss:0.1370
2025-01-12 15:54:21,186 - avg_ep_EM:0.8093
2025-01-12 15:54:21,186 - avg_ep_acc:0.9102
2025-01-12 15:54:21,186 - lr: 0.001926
2025-01-12 15:54:21,186 - Fin:Sun Jan 12 16:04:47 2025
2025-01-12 15:54:21,186 - ----------
2025-01-12 15:55:31,308 - Epoch: 6
2025-01-12 15:55:31,308 - Iter: 1800/ 2600
2025-01-12 15:55:31,308 - Loss:0.1428
2025-01-12 15:55:31,308 - avg_ep_EM:0.8132
2025-01-12 15:55:31,308 - avg_ep_acc:0.9120
2025-01-12 15:55:31,308 - lr: 0.001733
2025-01-12 15:55:31,308 - Fin:Sun Jan 12 16:04:47 2025
2025-01-12 15:55:31,309 - ----------
2025-01-12 15:55:45,504 - Gama of progressive dropout attention is: 0.86812553
2025-01-12 15:56:43,405 - Epoch: 7
2025-01-12 15:56:43,406 - Iter: 1900/ 2600
2025-01-12 15:56:43,406 - Loss:0.1410
2025-01-12 15:56:43,406 - avg_ep_EM:0.8045
2025-01-12 15:56:43,406 - avg_ep_acc:0.9077
2025-01-12 15:56:43,406 - lr: 0.001537
2025-01-12 15:56:43,406 - Fin:Sun Jan 12 16:04:51 2025
2025-01-12 15:56:43,406 - ----------
2025-01-12 15:57:53,336 - Epoch: 7
2025-01-12 15:57:53,336 - Iter: 2000/ 2600
2025-01-12 15:57:53,336 - Loss:0.1348
2025-01-12 15:57:53,337 - avg_ep_EM:0.8118
2025-01-12 15:57:53,337 - avg_ep_acc:0.9116
2025-01-12 15:57:53,337 - lr: 0.001338
2025-01-12 15:57:53,337 - Fin:Sun Jan 12 16:04:51 2025
2025-01-12 15:57:53,337 - ----------
2025-01-12 15:58:49,659 - Gama of progressive dropout attention is: 0.85076302
2025-01-12 15:59:05,848 - Epoch: 8
2025-01-12 15:59:05,848 - Iter: 2100/ 2600
2025-01-12 15:59:05,849 - Loss:0.1299
2025-01-12 15:59:05,849 - avg_ep_EM:0.8160
2025-01-12 15:59:05,849 - avg_ep_acc:0.9143
2025-01-12 15:59:05,849 - lr: 0.001136
2025-01-12 15:59:05,849 - Fin:Sun Jan 12 16:04:55 2025
2025-01-12 15:59:05,849 - ----------
2025-01-12 16:00:15,764 - Epoch: 8
2025-01-12 16:00:15,765 - Iter: 2200/ 2600
2025-01-12 16:00:15,765 - Loss:0.1356
2025-01-12 16:00:15,765 - avg_ep_EM:0.8251
2025-01-12 16:00:15,765 - avg_ep_acc:0.9179
2025-01-12 16:00:15,765 - lr: 0.000930
2025-01-12 16:00:15,765 - Fin:Sun Jan 12 16:04:55 2025
2025-01-12 16:00:15,765 - ----------
2025-01-12 16:01:25,488 - Epoch: 8
2025-01-12 16:01:25,488 - Iter: 2300/ 2600
2025-01-12 16:01:25,488 - Loss:0.1327
2025-01-12 16:01:25,488 - avg_ep_EM:0.8220
2025-01-12 16:01:25,488 - avg_ep_acc:0.9165
2025-01-12 16:01:25,488 - lr: 0.000718
2025-01-12 16:01:25,488 - Fin:Sun Jan 12 16:04:55 2025
2025-01-12 16:01:25,488 - ----------
2025-01-12 16:01:53,765 - Gama of progressive dropout attention is: 0.83374776
2025-01-12 16:02:37,874 - Epoch: 9
2025-01-12 16:02:37,875 - Iter: 2400/ 2600
2025-01-12 16:02:37,875 - Loss:0.1251
2025-01-12 16:02:37,875 - avg_ep_EM:0.8282
2025-01-12 16:02:37,875 - avg_ep_acc:0.9191
2025-01-12 16:02:37,875 - lr: 0.000499
2025-01-12 16:02:37,875 - Fin:Sun Jan 12 16:04:57 2025
2025-01-12 16:02:37,875 - ----------
2025-01-12 16:03:47,942 - Epoch: 9
2025-01-12 16:03:47,942 - Iter: 2500/ 2600
2025-01-12 16:03:47,942 - Loss:0.1287
2025-01-12 16:03:47,942 - avg_ep_EM:0.8279
2025-01-12 16:03:47,942 - avg_ep_acc:0.9195
2025-01-12 16:03:47,942 - lr: 0.000269
2025-01-12 16:03:47,943 - Fin:Sun Jan 12 16:04:57 2025
2025-01-12 16:03:47,943 - ----------
2025-01-12 16:04:58,151 - Epoch: 9
2025-01-12 16:04:58,151 - Iter: 2600/ 2600
2025-01-12 16:04:58,151 - Loss:0.1273
2025-01-12 16:04:58,151 - avg_ep_EM:0.8270
2025-01-12 16:04:58,151 - avg_ep_acc:0.9192
2025-01-12 16:04:58,151 - lr: 0.000004
2025-01-12 16:04:58,151 - Fin:Sun Jan 12 16:04:58 2025
2025-01-12 16:04:58,151 - ----------
2025-01-12 16:04:58,323 - Gama of progressive dropout attention is: 0.81707281
2025-01-12 16:05:01,591 - 0
2025-01-12 16:05:05,816 - 100
2025-01-12 16:05:10,116 - 200
2025-01-12 16:05:14,388 - 300
2025-01-12 16:05:14,980 - ===== Evaluation Scores =====
2025-01-12 16:05:14,980 - Pixel Accuracy: 0.8326
2025-01-12 16:05:14,980 - Mean Accuracy: 0.8314
2025-01-12 16:05:14,980 - Frequency Weighted IoU: 0.7143
2025-01-12 16:05:14,980 - Mean IoU: 0.7201
2025-01-12 16:05:14,980 - ===== Class IoU =====
2025-01-12 16:05:14,980 - Class 0: 0.7428
2025-01-12 16:05:14,980 - Class 1: 0.7502
2025-01-12 16:05:14,980 - Class 2: 0.7119
2025-01-12 16:05:14,980 - Class 3: 0.6756
