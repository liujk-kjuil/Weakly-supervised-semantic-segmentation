2025-01-12 15:11:47,053 - Arguments:
2025-01-12 15:11:47,054 - batch_size: 64
2025-01-12 15:11:47,054 - max_epoches: 20
2025-01-12 15:11:47,054 - network: network.resnet38_cls
2025-01-12 15:11:47,054 - lr: 0.01
2025-01-12 15:11:47,054 - num_workers: 10
2025-01-12 15:11:47,054 - wt_dec: 0.0005
2025-01-12 15:11:47,054 - session_name: Stage 1
2025-01-12 15:11:47,054 - env_name: PDA
2025-01-12 15:11:47,054 - model_name: PDA
2025-01-12 15:11:47,054 - n_class: 4
2025-01-12 15:11:47,054 - weights: init_weights/ilsvrc-cls_rna-a1_cls1000_ep-0001.params
2025-01-12 15:11:47,054 - trainroot: datasets/LUAD-HistoSeg/train/
2025-01-12 15:11:47,054 - testroot: datasets/LUAD-HistoSeg/test/
2025-01-12 15:11:47,054 - save_folder: checkpoints/
2025-01-12 15:11:47,054 - init_gama: 1
2025-01-12 15:11:47,054 - dataset: luad
2025-01-12 15:11:47,054 - log_dir: logs/stage1/exp08
2025-01-12 15:11:53,168 - Session started:
2025-01-12 15:11:53,168 - Sun Jan 12 15:11:53 2025
2025-01-12 15:13:02,125 - Epoch: 0
2025-01-12 15:13:02,126 - Iter:  100/ 5200
2025-01-12 15:13:02,126 - Loss:0.3489
2025-01-12 15:13:02,126 - avg_ep_EM:0.5606
2025-01-12 15:13:02,126 - avg_ep_acc:0.7494
2025-01-12 15:13:02,126 - lr: 0.0098
2025-01-12 15:13:02,126 - Fin:Sun Jan 12 16:11:38 2025
2025-01-12 15:14:09,081 - Epoch: 0
2025-01-12 15:14:09,081 - Iter:  200/ 5200
2025-01-12 15:14:09,081 - Loss:0.2702
2025-01-12 15:14:09,081 - avg_ep_EM:0.6220
2025-01-12 15:14:09,081 - avg_ep_acc:0.7977
2025-01-12 15:14:09,081 - lr: 0.0097
2025-01-12 15:14:09,081 - Fin:Sun Jan 12 16:10:46 2025
2025-01-12 15:14:49,418 - Gama of progressive dropout attention is:
2025-01-12 15:14:49,419 - 0.98
2025-01-12 15:15:18,408 - Epoch: 1
2025-01-12 15:15:18,409 - Iter:  300/ 5200
2025-01-12 15:15:18,409 - Loss:0.2350
2025-01-12 15:15:18,409 - avg_ep_EM:0.6589
2025-01-12 15:15:18,409 - avg_ep_acc:0.8252
2025-01-12 15:15:18,409 - lr: 0.0095
2025-01-12 15:15:18,409 - Fin:Sun Jan 12 16:11:10 2025
2025-01-12 15:16:25,435 - Epoch: 1
2025-01-12 15:16:25,436 - Iter:  400/ 5200
2025-01-12 15:16:25,436 - Loss:0.2182
2025-01-12 15:16:25,436 - avg_ep_EM:0.6995
2025-01-12 15:16:25,436 - avg_ep_acc:0.8502
2025-01-12 15:16:25,436 - lr: 0.0093
2025-01-12 15:16:25,436 - Fin:Sun Jan 12 16:10:52 2025
2025-01-12 15:17:32,637 - Epoch: 1
2025-01-12 15:17:32,638 - Iter:  500/ 5200
2025-01-12 15:17:32,638 - Loss:0.1968
2025-01-12 15:17:32,638 - avg_ep_EM:0.7176
2025-01-12 15:17:32,638 - avg_ep_acc:0.8601
2025-01-12 15:17:32,638 - lr: 0.0091
2025-01-12 15:17:32,638 - Fin:Sun Jan 12 16:10:43 2025
2025-01-12 15:17:46,236 - Gama of progressive dropout attention is:
2025-01-12 15:17:46,236 - 0.9603999999999999
2025-01-12 15:18:42,348 - Epoch: 2
2025-01-12 15:18:42,348 - Iter:  600/ 5200
2025-01-12 15:18:42,348 - Loss:0.1870
2025-01-12 15:18:42,348 - avg_ep_EM:0.7478
2025-01-12 15:18:42,348 - avg_ep_acc:0.8774
2025-01-12 15:18:42,348 - lr: 0.0090
2025-01-12 15:18:42,348 - Fin:Sun Jan 12 16:10:59 2025
2025-01-12 15:19:49,542 - Epoch: 2
2025-01-12 15:19:49,543 - Iter:  700/ 5200
2025-01-12 15:19:49,543 - Loss:0.1811
2025-01-12 15:19:49,543 - avg_ep_EM:0.7555
2025-01-12 15:19:49,543 - avg_ep_acc:0.8817
2025-01-12 15:19:49,543 - lr: 0.0088
2025-01-12 15:19:49,543 - Fin:Sun Jan 12 16:10:51 2025
2025-01-12 15:20:43,420 - Gama of progressive dropout attention is:
2025-01-12 15:20:43,420 - 0.9411919999999999
2025-01-12 15:20:59,738 - Epoch: 3
2025-01-12 15:20:59,739 - Iter:  800/ 5200
2025-01-12 15:20:59,739 - Loss:nan
2025-01-12 15:20:59,739 - avg_ep_EM:0.6520
2025-01-12 15:20:59,739 - avg_ep_acc:0.7795
2025-01-12 15:20:59,739 - lr: 0.0086
2025-01-12 15:20:59,739 - Fin:Sun Jan 12 16:11:05 2025
2025-01-12 15:22:09,486 - Epoch: 3
2025-01-12 15:22:09,486 - Iter:  900/ 5200
2025-01-12 15:22:09,486 - Loss:nan
2025-01-12 15:22:09,486 - avg_ep_EM:0.0263
2025-01-12 15:22:09,486 - avg_ep_acc:0.0505
2025-01-12 15:22:09,486 - lr: 0.0084
2025-01-12 15:22:09,487 - Fin:Sun Jan 12 16:11:14 2025
2025-01-12 15:23:19,434 - Epoch: 3
2025-01-12 15:23:19,435 - Iter: 1000/ 5200
2025-01-12 15:23:19,435 - Loss:nan
2025-01-12 15:23:19,435 - avg_ep_EM:0.0090
2025-01-12 15:23:19,435 - avg_ep_acc:0.0172
2025-01-12 15:23:19,435 - lr: 0.0083
2025-01-12 15:23:19,435 - Fin:Sun Jan 12 16:11:21 2025
2025-01-12 15:23:47,515 - Gama of progressive dropout attention is:
2025-01-12 15:23:47,516 - 0.9223681599999999
2025-01-12 15:24:31,182 - Epoch: 4
2025-01-12 15:24:31,183 - Iter: 1100/ 5200
2025-01-12 15:24:31,183 - Loss:nan
2025-01-12 15:24:31,183 - avg_ep_EM:0.0025
2025-01-12 15:24:31,183 - avg_ep_acc:0.0048
2025-01-12 15:24:31,183 - lr: 0.0081
2025-01-12 15:24:31,183 - Fin:Sun Jan 12 16:11:36 2025
2025-01-12 15:25:40,855 - Epoch: 4
2025-01-12 15:25:40,856 - Iter: 1200/ 5200
2025-01-12 15:25:40,856 - Loss:nan
2025-01-12 15:25:40,856 - avg_ep_EM:0.0000
2025-01-12 15:25:40,856 - avg_ep_acc:0.0000
2025-01-12 15:25:40,856 - lr: 0.0079
2025-01-12 15:25:40,856 - Fin:Sun Jan 12 16:11:39 2025
2025-01-12 15:26:50,436 - Epoch: 4
2025-01-12 15:26:50,437 - Iter: 1300/ 5200
2025-01-12 15:26:50,437 - Loss:nan
2025-01-12 15:26:50,437 - avg_ep_EM:0.0000
2025-01-12 15:26:50,437 - avg_ep_acc:0.0000
2025-01-12 15:26:50,437 - lr: 0.0077
2025-01-12 15:26:50,437 - Fin:Sun Jan 12 16:11:42 2025
2025-01-12 15:26:50,658 - Gama of progressive dropout attention is:
2025-01-12 15:26:50,659 - 0.9039207967999998
2025-01-12 15:28:02,524 - Epoch: 5
2025-01-12 15:28:02,524 - Iter: 1400/ 5200
2025-01-12 15:28:02,524 - Loss:nan
2025-01-12 15:28:02,524 - avg_ep_EM:0.0000
2025-01-12 15:28:02,524 - avg_ep_acc:0.0000
2025-01-12 15:28:02,524 - lr: 0.0075
2025-01-12 15:28:02,524 - Fin:Sun Jan 12 16:11:53 2025
