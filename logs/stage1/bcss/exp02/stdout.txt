2025-01-12 22:20:45,379 - Arguments:
2025-01-12 22:20:45,379 - batch_size: 64
2025-01-12 22:20:45,379 - max_epoches: 20
2025-01-12 22:20:45,379 - network: network.resnet38_cls
2025-01-12 22:20:45,379 - lr: 0.01
2025-01-12 22:20:45,379 - num_workers: 10
2025-01-12 22:20:45,379 - wt_dec: 0.0005
2025-01-12 22:20:45,379 - session_name: Stage 1
2025-01-12 22:20:45,379 - env_name: PDA
2025-01-12 22:20:45,379 - model_name: PDA
2025-01-12 22:20:45,379 - n_class: 4
2025-01-12 22:20:45,379 - weights: init_weights/ilsvrc-cls_rna-a1_cls1000_ep-0001.params
2025-01-12 22:20:45,379 - trainroot: datasets/BCSS-WSSS/train/
2025-01-12 22:20:45,379 - testroot: datasets/BCSS-WSSS/test/
2025-01-12 22:20:45,379 - save_folder: checkpoints/
2025-01-12 22:20:45,379 - init_gama: 1
2025-01-12 22:20:45,379 - dataset: bcss
2025-01-12 22:20:45,379 - log_dir: logs/stage1/bcss/exp02
2025-01-12 22:20:45,379 - ----------
2025-01-12 22:20:50,888 - Session started:
2025-01-12 22:20:50,889 - Sun Jan 12 22:20:50 2025
2025-01-12 22:21:58,437 - Epoch: 0
2025-01-12 22:21:58,437 - Iter:  100/ 7300
2025-01-12 22:21:58,437 - Loss:0.4869
2025-01-12 22:21:58,437 - avg_ep_EM:0.3161
2025-01-12 22:21:58,437 - avg_ep_acc:0.5842
2025-01-12 22:21:58,437 - lr: 0.009878
2025-01-12 22:21:58,437 - Fin:Sun Jan 12 23:43:01 2025
2025-01-12 22:21:58,437 - ----------
2025-01-12 22:23:04,531 - Epoch: 0
2025-01-12 22:23:04,531 - Iter:  200/ 7300
2025-01-12 22:23:04,531 - Loss:0.3879
2025-01-12 22:23:04,531 - avg_ep_EM:0.4032
2025-01-12 22:23:04,531 - avg_ep_acc:0.6569
2025-01-12 22:23:04,531 - lr: 0.009754
2025-01-12 22:23:04,531 - Fin:Sun Jan 12 23:42:08 2025
2025-01-12 22:23:04,531 - ----------
2025-01-12 22:24:10,707 - Epoch: 0
2025-01-12 22:24:10,707 - Iter:  300/ 7300
2025-01-12 22:24:10,707 - Loss:0.3394
2025-01-12 22:24:10,707 - avg_ep_EM:0.4459
2025-01-12 22:24:10,707 - avg_ep_acc:0.6884
2025-01-12 22:24:10,707 - lr: 0.009631
2025-01-12 22:24:10,707 - Fin:Sun Jan 12 23:41:53 2025
2025-01-12 22:24:10,707 - ----------
2025-01-12 22:24:53,819 - Gama of progressive dropout attention is: 0.98000000
2025-01-12 22:25:18,983 - Epoch: 1
2025-01-12 22:25:18,983 - Iter:  400/ 7300
2025-01-12 22:25:18,983 - Loss:0.3192
2025-01-12 22:25:18,983 - avg_ep_EM:0.4898
2025-01-12 22:25:18,983 - avg_ep_acc:0.7231
2025-01-12 22:25:18,983 - lr: 0.009507
2025-01-12 22:25:18,983 - Fin:Sun Jan 12 23:42:23 2025
2025-01-12 22:25:18,983 - ----------
2025-01-12 22:26:25,504 - Epoch: 1
2025-01-12 22:26:25,505 - Iter:  500/ 7300
2025-01-12 22:26:25,505 - Loss:0.3093
2025-01-12 22:26:25,505 - avg_ep_EM:0.5621
2025-01-12 22:26:25,505 - avg_ep_acc:0.7724
2025-01-12 22:26:25,505 - lr: 0.009383
2025-01-12 22:26:25,505 - Fin:Sun Jan 12 23:42:16 2025
2025-01-12 22:26:25,505 - ----------
2025-01-12 22:27:31,946 - Epoch: 1
2025-01-12 22:27:31,946 - Iter:  600/ 7300
2025-01-12 22:27:31,946 - Loss:0.2966
2025-01-12 22:27:31,946 - avg_ep_EM:0.5688
2025-01-12 22:27:31,946 - avg_ep_acc:0.7774
2025-01-12 22:27:31,946 - lr: 0.009258
2025-01-12 22:27:31,946 - Fin:Sun Jan 12 23:42:10 2025
2025-01-12 22:27:31,947 - ----------
2025-01-12 22:28:38,296 - Epoch: 1
2025-01-12 22:28:38,296 - Iter:  700/ 7300
2025-01-12 22:28:38,296 - Loss:0.2885
2025-01-12 22:28:38,297 - avg_ep_EM:0.5760
2025-01-12 22:28:38,297 - avg_ep_acc:0.7818
2025-01-12 22:28:38,297 - lr: 0.009134
2025-01-12 22:28:38,297 - Fin:Sun Jan 12 23:42:05 2025
2025-01-12 22:28:38,297 - ----------
2025-01-12 22:28:58,355 - Gama of progressive dropout attention is: 0.96040000
2025-01-12 22:29:46,331 - Epoch: 2
2025-01-12 22:29:46,331 - Iter:  800/ 7300
2025-01-12 22:29:46,331 - Loss:0.2761
2025-01-12 22:29:46,331 - avg_ep_EM:0.6053
2025-01-12 22:29:46,331 - avg_ep_acc:0.7994
2025-01-12 22:29:46,331 - lr: 0.009009
2025-01-12 22:29:46,331 - Fin:Sun Jan 12 23:42:16 2025
2025-01-12 22:29:46,331 - ----------
2025-01-12 22:30:52,623 - Epoch: 2
2025-01-12 22:30:52,623 - Iter:  900/ 7300
2025-01-12 22:30:52,623 - Loss:0.2752
2025-01-12 22:30:52,623 - avg_ep_EM:0.6054
2025-01-12 22:30:52,624 - avg_ep_acc:0.8018
2025-01-12 22:30:52,624 - lr: 0.008884
2025-01-12 22:30:52,624 - Fin:Sun Jan 12 23:42:11 2025
2025-01-12 22:30:52,624 - ----------
2025-01-12 22:31:58,998 - Epoch: 2
2025-01-12 22:31:58,999 - Iter: 1000/ 7300
2025-01-12 22:31:58,999 - Loss:0.2715
2025-01-12 22:31:58,999 - avg_ep_EM:0.6084
2025-01-12 22:31:58,999 - avg_ep_acc:0.8037
2025-01-12 22:31:58,999 - lr: 0.008759
2025-01-12 22:31:58,999 - Fin:Sun Jan 12 23:42:08 2025
2025-01-12 22:31:58,999 - ----------
2025-01-12 22:33:02,135 - Gama of progressive dropout attention is: 0.94119200
2025-01-12 22:33:07,076 - Epoch: 3
2025-01-12 22:33:07,076 - Iter: 1100/ 7300
2025-01-12 22:33:07,076 - Loss:0.3142
2025-01-12 22:33:07,076 - avg_ep_EM:0.5999
2025-01-12 22:33:07,076 - avg_ep_acc:0.7948
2025-01-12 22:33:07,076 - lr: 0.008634
2025-01-12 22:33:07,076 - Fin:Sun Jan 12 23:42:16 2025
2025-01-12 22:33:07,076 - ----------
2025-01-12 22:34:15,771 - Epoch: 3
2025-01-12 22:34:15,771 - Iter: 1200/ 7300
2025-01-12 22:34:15,771 - Loss:0.6932
2025-01-12 22:34:15,771 - avg_ep_EM:0.0300
2025-01-12 22:34:15,771 - avg_ep_acc:0.0544
2025-01-12 22:34:15,771 - lr: 0.008509
2025-01-12 22:34:15,771 - Fin:Sun Jan 12 23:42:27 2025
2025-01-12 22:34:15,771 - ----------
2025-01-12 22:35:24,538 - Epoch: 3
2025-01-12 22:35:24,538 - Iter: 1300/ 7300
2025-01-12 22:35:24,538 - Loss:0.6931
2025-01-12 22:35:24,538 - avg_ep_EM:0.0068
2025-01-12 22:35:24,538 - avg_ep_acc:0.0123
2025-01-12 22:35:24,538 - lr: 0.008383
2025-01-12 22:35:24,538 - Fin:Sun Jan 12 23:42:36 2025
2025-01-12 22:35:24,538 - ----------
2025-01-12 22:36:33,514 - Epoch: 3
2025-01-12 22:36:33,515 - Iter: 1400/ 7300
2025-01-12 22:36:33,515 - Loss:0.6931
2025-01-12 22:36:33,515 - avg_ep_EM:0.0040
2025-01-12 22:36:33,515 - avg_ep_acc:0.0073
2025-01-12 22:36:33,515 - lr: 0.008257
2025-01-12 22:36:33,515 - Fin:Sun Jan 12 23:42:46 2025
2025-01-12 22:36:33,515 - ----------
2025-01-12 22:37:14,973 - Gama of progressive dropout attention is: 0.92236816
2025-01-12 22:37:44,140 - Epoch: 4
2025-01-12 22:37:44,140 - Iter: 1500/ 7300
2025-01-12 22:37:44,141 - Loss:0.6931
2025-01-12 22:37:44,141 - avg_ep_EM:0.0018
2025-01-12 22:37:44,141 - avg_ep_acc:0.0033
2025-01-12 22:37:44,141 - lr: 0.008131
2025-01-12 22:37:44,141 - Fin:Sun Jan 12 23:43:02 2025
2025-01-12 22:37:44,141 - ----------
2025-01-12 22:38:53,174 - Epoch: 4
2025-01-12 22:38:53,175 - Iter: 1600/ 7300
2025-01-12 22:38:53,175 - Loss:0.6931
2025-01-12 22:38:53,175 - avg_ep_EM:0.0001
2025-01-12 22:38:53,175 - avg_ep_acc:0.0001
2025-01-12 22:38:53,175 - lr: 0.008005
2025-01-12 22:38:53,175 - Fin:Sun Jan 12 23:43:08 2025
2025-01-12 22:38:53,175 - ----------
2025-01-12 22:40:02,282 - Epoch: 4
2025-01-12 22:40:02,283 - Iter: 1700/ 7300
2025-01-12 22:40:02,283 - Loss:0.6931
2025-01-12 22:40:02,283 - avg_ep_EM:0.0010
2025-01-12 22:40:02,283 - avg_ep_acc:0.0016
2025-01-12 22:40:02,283 - lr: 0.007879
2025-01-12 22:40:02,283 - Fin:Sun Jan 12 23:43:15 2025
2025-01-12 22:40:02,283 - ----------
2025-01-12 22:41:11,105 - Epoch: 4
2025-01-12 22:41:11,105 - Iter: 1800/ 7300
2025-01-12 22:41:11,105 - Loss:0.6963
2025-01-12 22:41:11,106 - avg_ep_EM:0.0047
2025-01-12 22:41:11,106 - avg_ep_acc:0.0087
2025-01-12 22:41:11,106 - lr: 0.007752
2025-01-12 22:41:11,106 - Fin:Sun Jan 12 23:43:19 2025
2025-01-12 22:41:11,106 - ----------
2025-01-12 22:41:28,480 - Gama of progressive dropout attention is: 0.90392080
2025-01-12 22:42:21,642 - Epoch: 5
2025-01-12 22:42:21,643 - Iter: 1900/ 7300
2025-01-12 22:42:21,643 - Loss:0.6931
2025-01-12 22:42:21,643 - avg_ep_EM:0.0010
2025-01-12 22:42:21,643 - avg_ep_acc:0.0018
2025-01-12 22:42:21,643 - lr: 0.007625
2025-01-12 22:42:21,643 - Fin:Sun Jan 12 23:43:30 2025
2025-01-12 22:42:21,643 - ----------
2025-01-12 22:43:30,602 - Epoch: 5
2025-01-12 22:43:30,603 - Iter: 2000/ 7300
2025-01-12 22:43:30,603 - Loss:0.6931
2025-01-12 22:43:30,603 - avg_ep_EM:0.0000
2025-01-12 22:43:30,603 - avg_ep_acc:0.0000
2025-01-12 22:43:30,603 - lr: 0.007498
2025-01-12 22:43:30,603 - Fin:Sun Jan 12 23:43:33 2025
2025-01-12 22:43:30,603 - ----------
2025-01-12 22:44:39,397 - Epoch: 5
2025-01-12 22:44:39,397 - Iter: 2100/ 7300
2025-01-12 22:44:39,397 - Loss:0.6931
2025-01-12 22:44:39,397 - avg_ep_EM:0.0000
2025-01-12 22:44:39,397 - avg_ep_acc:0.0000
2025-01-12 22:44:39,397 - lr: 0.007370
2025-01-12 22:44:39,397 - Fin:Sun Jan 12 23:43:36 2025
2025-01-12 22:44:39,397 - ----------
2025-01-12 22:45:41,566 - Gama of progressive dropout attention is: 0.88584238
2025-01-12 22:45:50,117 - Epoch: 6
2025-01-12 22:45:50,117 - Iter: 2200/ 7300
2025-01-12 22:45:50,117 - Loss:0.6931
2025-01-12 22:45:50,117 - avg_ep_EM:0.0000
2025-01-12 22:45:50,117 - avg_ep_acc:0.0000
2025-01-12 22:45:50,117 - lr: 0.007243
2025-01-12 22:45:50,117 - Fin:Sun Jan 12 23:43:45 2025
2025-01-12 22:45:50,117 - ----------
2025-01-12 22:46:59,303 - Epoch: 6
2025-01-12 22:46:59,303 - Iter: 2300/ 7300
2025-01-12 22:46:59,303 - Loss:0.6931
2025-01-12 22:46:59,303 - avg_ep_EM:0.0000
2025-01-12 22:46:59,303 - avg_ep_acc:0.0000
2025-01-12 22:46:59,303 - lr: 0.007115
2025-01-12 22:46:59,303 - Fin:Sun Jan 12 23:43:48 2025
2025-01-12 22:46:59,303 - ----------
